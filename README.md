ðŸŽ“ Analyzing Academic Biases Using "Rate My Professors" Data ðŸ“Š - for Computational Statistics

For this project, Tais, Deirdre and I analyzed a dataset of over 1,400 professors and 20,000 student reviews from Rate My Professors to explore trends and biases in academia. Hereâ€™s what we did:

Data Preparation:
Consolidated individual reviews to averages per professor.
Cleaned the dataset to focus on statistically robust samples (â‰¥8 reviews per professor).

Key Analyses:
Rating Trends: Confirmed star ratings follow a Beta distribution. Students' strong opinions drove ratings to extremes, but difficulty ratings were normally distributed.
Predictive Modeling: Built regression models with key explanatory variables like difficulty rating, valuable feedback, and lecture style. The best model explained only ~34% of rating variability, highlighting individual nuances.

Bias Exploration:
Gender and race showed limited impact on ratings or tags like "caring" and "respected."
Race influenced teaching location and tags like "inspirational." Non-white professors were more likely tagged as "inspirational" and found at community colleges.
STEM vs. Non-STEM: STEM professors had slightly lower ratings and were more represented at community colleges than non-STEM professors.

Conclusions:
The dataset reflected biases in academia but was limited in scope and completeness.
While statistical significance was found in some areas, broader representation and more comprehensive data are crucial for deeper insights.
This study underscores how platforms like Rate My Professors reveal both student perceptions and systemic academic trends.
